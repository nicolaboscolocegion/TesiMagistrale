%!TEX root = ../main.tex

\chapter{Implementation}
\label{chp:project}
\noindent
This chapter will explain the implemented software solution.
\section{The 3D models}
\noindent
In this section will talk about how 3D models are saved and rendered on \ac{UE}
\subsection{The OBJ file format}
\noindent
For understanding how \ac{UE} will show 3D models and how they will be stored, we must talk about their characteristics.

\begin{itemize}
    \item \textbf{Vertices:} points that describe the geometry
    \item \textbf{Faces} indicated were there is a polygon by grouping 3 or more vertices 
    \item \textbf{Normal:} there is one for each vertex that is in a face, indicates the direction to which the face is exposed, and for calculating how light is reflected
    \item \textbf{UV:} vectors that helps how a texture should be applied to the model
    \item \textbf{Vertex colors:} RGB vector that indicates a color for each vertex
\end{itemize}
\noindent
As we talked about in chapter \ref{chp:Requirements}, we will use the OBJ file format for storing files.
The React Three Fiber has a native support for OBJ, and the backend server does not need to read the file but just to manage by saving, deleting and sending it via \ac{HTTP}.
Unfortunately \ac{UE} does not have a OBJ file reader usable at runtime but just an importer for what it calls static meshes (3D models that don't have moving parts).
So there is the need to build a parser Alg.[\ref{alg:parser}] OBJ to \ac{UE} custom types.\\
First we need to understand how the OBJ file format is compose of, here a general example code:\ref{code:OBJExample}

\lstinputlisting[float=ht, language=Octave, caption=OBJ file example,style=obj, label={code:OBJExample}]{code/exampleOBJ.txt}
\noindent
The vertices, UV and normals are simply written, instead the faces have different formats, first not always they use triangles, but also quads, this depends on how the file was exported.
For ease of use the parser will support both. The numbers of the face are the indices of the vertex. Indices start from 1.
A face can also have the UV and normals corresponding for the vertex. For our use cases UV aren't needed, but for future-proof they are still being parsed correctly.\\
Sometimes it is useful to divide the 3D model into multiple objects the OBJ format represents by dividing the 3D model with a "\texttt{o}".
OBJ can also divide the faces in groups by dividing them with a "\texttt{g}".
Here an example of how the division works: code:\ref{code:OBJgrouping}

\lstinputlisting[float=ht, language=Octave, caption=OBJ grouping,style=obj, label={code:OBJgrouping}]{code/exampleOBJGrups.txt}
\noindent
The software that the surgeons are using for exporting 3D models just support groups, so we will implement those, and they will become useful for rendering the model in parts.

\subsection{To Unreal types}
\noindent
\ac{UE} has some custom classes for managing things like vectors, colors... These classes also have useful methods that also interface with the blueprint system, we can for example expose variables or functions, so we can call them at blueprint level.
This is very important so that we can interconnect the \texttt{C++} components with blueprints.\\
Unreal has a component called \texttt{procedural mesh}, this component has the possibility to render a 3D model given vertices and triangles, it also has more data that you can feed to the rendered mesh such as: normals, tangents, UV, vertex colors.
It can also have collisions and a material.\\
The \texttt{procedural mesh} also has the possibility to load the mesh in parts, so the parser will save the different triangles in the various groups that are defined in the file.
This will be important later for loading time.\\
Vertices are directly read and saved in an array of \texttt{FVector} and normals will be saved in the same way.
Vertex colors just need to be read and put in a \texttt{LinearColor} array, the object itself can be initialized with the data retrieved in the file.
UV because they are 2D vectors will be saved in an array of \texttt{Vector2D}.
Unfortunately there is a mismatch between how \ac{UE} manages correlation between vertices and normals with respect to the OBJ file standard.
Unreal needs two arrays that contain vertices and normals, so that the vertex in the array at the position \texttt{i} must have its normal in the normal array at position \texttt{i}. 
This is still a trivial problem, because there's just the need to load all the normals in memory and then save them again in the correct order decided by the faces.\\
UVs are being managed in the same way.
Another problem is that unreal just accepts triangles and not quads, and because it is a common practice to use quads when exporting 3D models the program will convert quads in triangles, this is pretty trivial, for each quad we can divide it in two triangles.\\
Unreal also works in \texttt{Z}-up coordinates that means that the \texttt{Z} axis points up, there is another standard called \texttt{Y}-up were the \texttt{Y} axis points up, unfortunately the OBJ file format does not have any ways to reference scale or if the file is saved in \texttt{Z}-up or \texttt{Y}-up,
so it is important to export the file in \texttt{Z}-up.\\

\input{code/pseudoCode/parser}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{blueprints/loadingMesh.png}
    \caption{Loading mesh funcion}
    \label{fig:loadingMesh}
\end{figure}
\noindent
To load the mesh, I created a recursive function Fig.[\ref{fig:loadingMesh}] that loads each mesh group every 0.5 ms. A delay is necessary because some 3D models are large and can significantly impact performance during loading, and this helps to mitigate that. The function needs to be recursive because \ac{UE} loops do not allow delays within them.
After all the mesh groups are loaded, another similar function is called to change the material and display the 3D mesh colors.\\
The final result can be seen in the first chapter in Fig.[\ref{fig:heart}].
\subsection{3DmodelVewer actor}
\noindent
The \texttt{3DModelActor} is a custom actor designed to facilitate the visualization of 3D heart models that is an extension of \texttt{Grippable Actor}. This actor comprises four primary components. The first component is the Procedural Mesh, which utilizes Unreal Engine's capabilities to generate and display a heart model in real-time. 
The second component is a Loading Indicator \texttt{Sphere} that provides visual feedback during model loading processes informing users about the ongoing operations. In addition, the \texttt{3DModelActor} features a \texttt{Text Component} that displays error messages when issues arise during the loading of the 3D models.\\
The core functionality of the actor is driven by a custom \texttt{Model Loader}, This custom component is responsible for fetching 3D models from the server, parsing the model data, and integrating it into the procedural mesh and it implements the OBJ parser.\\
The \texttt{Model Loader component} can download the 3D Model from the backend thanks to the function httpFileDownload, that thanks to a delegate, can be runned in an asynchronous way thanks to \ac{UE} threads, and then notify when the execution is complete.
Another functionality of \texttt{3DModelActor} is that it can be gripped thanks to its parent Blueprint \texttt{Grippable Actor}.

\lstinputlisting[language=C++, caption=Model loader header file, label={code:model loader}, linerange={12-74}, style=cpp]{code/c++/model loader.h}

\paragraph{Resizable functionality}
As specified in the requirements, the 3D models must be resizable. Therefore, the actor needs an event that allows its size to be changed. This event must be an \ac{RPCs} that is executed on the server, ensuring that the action is replicated across all clients.

\section{The VR Character}
\noindent
Thanks to the VRExpansion plugin, we can use the standard \texttt{VR Character}, this \texttt{Character} has already implemented the online synchronization, and it also has the components for the controllers and camera management.
The controllers can \texttt{grip} any \texttt{Actor} or \texttt{Component} that implements the interface \texttt{VRGrip}, this will be used for  moving the 3D models.
Other than that the \texttt{VR Character} is an empty blank, and will need to implement some functions for making it fully functional.\\
Here are the main components to develop:

\begin{itemize}
    \item Input management
    \item Widget Interaction
    \item Interaction pointer
    \item Side menu
    \item Grip framework
    \item 3D model size management    
    \item Loading sphere
\end{itemize}

\subsection{Input management}
\noindent
Input in \ac{UE} is managed by two data files: \texttt{Input Action}and \texttt{Input Mapping Context}.
In the next two paragraphs will be addressed how the input works, and then will be used in the various components of \texttt{VR Character}.

\paragraph{Input Action}
Are files that are used to identify a certain input of the controller, each file should be named after an action more than the input used for making clear what they serve.
For example:\\
For using the \texttt{A} button found in the right controller,
you need to have a file that represents the button,
the necessary settings are: Consume input which allows you to take into account that the input has been served,
and the type of value that in this case will be \texttt{Digital (bool)}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{blueprints/InputActionExample.png}
    \caption{Input Action example}
    \label{fig:InputAction}
\end{figure}

\paragraph{Input Mapping Context}
Files represent all the inputs used by an actor, an actor could change its inputs, so they can be multiple files for different occasions,
Here each \texttt{Input Action} will be associated with the corresponding input.
\texttt{Input Mapping Context} can be used for other objects so that they can override the standard behavior of the \texttt{Character}, for example by equipping a laser pointer and using the \texttt{A} button for toggle it.
Each \texttt{Input Mapping Context} can be bound with different controllers, this can be useful if we will be porting the app for another \ac{HMD}.
For setting the \texttt{Input Mapping Context} there is a function called \texttt{Add Mapping Context}.

\paragraph{Input animations}
Thanks to the rigged 3D models of the controllers provided by Meta, it is possible to import them as \texttt{skeletal meshes}.\\
A \texttt{skeletal} mesh consists of a mesh with an underlying skeleton, allowing it to flex, stretch, or rotate its bones to create animations.
We use these models to animate the controllers, simulating the user's inputs. This allows for visual representation of buttons being pressed, sticks being tilted, and the degree to which triggers and grips are engaged.
These animations are controlled by functions that read the input and adjust the position or rotation of the \texttt{skeletal} mesh bones accordingly.


\subsection{Widget Interaction}
\label{chp:widgetInteraction}
\noindent 
In a normal application we are used to managing input mainly via mouse or touch screens, in \ac{VR} we can not have that, so It is important to create some kind of \ac{UI}.
One of the most used approaches is showing some kind of virtual display with buttons so that the user can interact, for this will be using a blueprint called \texttt{Widget} and will be explained in chapter \ref{sec:widgets}.
Unfortunately \texttt{Widgets} are used for 2D menus but thanks to an actor component we can use it in a 3D environment.\\
For interacting with a \texttt{widget} in a 3D space, \ac{UE} has a component called \texttt{widget interaction} that can evaluate if it is pointing to a \texttt{widget}, it can also give the world location of where it is pointing. This component will be attached to each \texttt{grip motion controller component}.
For letting the user see exactly where the controller is pointed, when the controller is near the \texttt{Widget} a trace will be shown.
For the trace will be used a Component called \texttt{Spline Mesh}, as the name suggests, uses various points and interpolates a mesh for creating a complex curve.
Still our use will be simply by just using two points Fig.[\ref{fig:splineExample}]. So the algorithm is simple: each tick a function called \texttt{InteractionPointer} will control both controllers if the \texttt{widget interaction} points to a \texttt{Widget}, then will draw the spline.



\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{blueprints/interactionPointer.png}
    \caption{Widget Interaction code}
    \label{fig:InteractionPointer}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{vrScreenshot/splineExample.png}
    \caption{On the left a controller that create the spline pointing to the widget}
    \label{fig:splineExample}
\end{figure}

\subsection{Laser pointer}
\noindent
To create an effective laser pointer, we can simply use a red spline that starts from the controller and ends on the pointed surface. The code is very similar to the \texttt{widget interaction pointer} described in Chapter [\ref{chp:widgetInteraction}]. However, to find the pointer's location, we can use the line trace function, which detects the nearest point where a geometry intersects with an input segment.\\
The laser pointer will be an actor, and using a component called a child actor, we can attach it to the \texttt{VR Character} right controller. This is necessary to eliminate the latency effect caused by the tick group of the \texttt{VR Character}, which is set to \texttt{Pre Physic}. The laser pointer actor will stay in the \texttt{post update work}, allowing it to be ticked one frame before anything else.\\
The final result is identical to the \texttt{widget interaction pointer} shown in Fig.[\ref{fig:splineExample}], but the color will be red, and it will be toggled using the \texttt{A button}.

\subsection{Grippable system}
\noindent
Thanks to VRExpansion, we can use a grip function that allows us to attach an actor implementing the \text{VRGripInterface}.
The only challenge is determining which element should be gripped, but \ac{UE} provides a function called Sphere Trace for Objects, which returns the nearest actor within a specified radius.
If the actor implements the \texttt{VRGripInterface}, it can be gripped.\\
The grip is initiated when the controller grip button is pressed, and the object is dropped when the button is fully released.
To avoid conflicts, if both controllers attempt to grip the same object, the object will be dropped when the second controller tries to grip it.

\paragraph{Resizable actor}
For implementing the resizable functionality, all actors that are resizable will implement the \texttt{Resizable actor} interface, that will implement the resizable event.\\
When two controllers will try to grip the same actor, they will drop it, but if both grips are still pressed, the action of moving the controllers closer and further away will allow you to decide the new size of the actor.
For exiting this mode, one controller must release the grip button.\\
The code for this action is simple, when it is decided that the actor is been resized there the distance between the two controllers will be calculated and the current scale vector of the actor will be saved, after that the size will be decided by the following formula [\ref{math:ratio}], for smoothness, it will be executed on every tick.

\begin{equation}
    \label{math:ratio}
    New Size=Starting Actor Size \frac{Controllers Distance}{Starting Contrllers Distance}
\end{equation}

\subsection{Fade camera}
\label{Chp:fade}
\noindent
As we will explain in Chapter [\ref{chp:performance}], the average \ac{VR} user can be very susceptible to sudden movements, such as teleportation or moving in 3D space using the analog stick.
To mitigate the discomfort caused by sudden teleportation, every time a \texttt{VR Character} is teleported or loaded into another level, a fade-in and fade-out animation is applied.\\
This effect would be relatively simple to achieve using post-processing. However, for mobile development, post-processing is generally discouraged due to its high resource consumption. Therefore, it is important to adopt a less resource-intensive solution. Fortunately, \ac{UE} materials can be manipulated in real time.
By using a sphere with a specific material that has \texttt{depth test} disabled (so it renders on top of everything), and by animating the fade effect through a parameter controlling opacity.\\
The code will be implemented directly in the \texttt{VR Character}. The sphere will be spawned and destroyed as needed, and the animation will be controlled by a built-in function that changes the opacity value over a specified animation time.\\
Another minor issue is that when the player enters a new session, they will initially spawn at coordinates \texttt{X=0, Y=0, Z=0}, where code execution may not yet be possible. To address this, a sphere similar to the one used for the fade effect will be placed at those coordinates in multiplayer levels.\\
This sphere will be destroyed once the player has been relocated to the correct position. This sphere can be seen in Fig.[\ref{fig:classroom}].


\section{Widgets}
\label{sec:widgets}
\noindent
Widgets are Blueprints used for creating User interfaces in \ac{UE}, thanks to a lot of pre-made components such as: buttons, textbox, spacers… Each widget can have its own logic built with the Blueprint system.\\
For the app there will be 3 Menus:
\begin{itemize}
    \item \textbf{Main menu:} used for creating a session and joining it. It will have a number pad for inserting the code to enter a session.
    \item \textbf{Side menu:} used as a summable menù that will be displayed over the left controller, used for exiting the session or letting the host use the centering functionality.
    \item \textbf{3d Model picker:} used for selecting what 3D model the host wants to visualize. It will have a dynamically scroll bar section, so that it can load all the names of the models loaded in the server. 
\end{itemize}
\noindent
Unfortunately, widgets are normally used for 2D interfaces, but thanks to an actor component it is possible to visualize them in a 3D space, and is it possible to interact with them thanks to the \texttt{widget interaction component}.\\
Because the menus have to do network calls, they will be able to show error messages and have a loading animation, for cosmetic purposes, the animation will be a schema of a beating heart.


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.96\textwidth]{blueprints/mainMenu.png}
    \caption{Main menu widget}
    \label{fig:mainMenu}
\end{figure}

\subsection{HTTP request}
\noindent
For compatibility reasons with the \ac{HMD}, I built a blueprint function for \ac{HTTP} requests.
Because there is an internet call there is the need to do it in background, fortunately unreal gives a special \texttt{C++} class called \texttt{UBlueprintAsyncActionBase},
if we inherit it, we can create what in \ac{UE} is called \texttt{latent node} Fig.[\ref{fig:HTTPrequest}], these nodes can activate their output pins without stopping the frame until they are complete. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.4\textwidth]{blueprints/httpRequest.png}
    \caption{HTTP request node}
    \label{fig:HTTPrequest}
\end{figure}
\noindent
Because the app does not need a full \ac{HTTP} implementation we will implement the block with this parameters an outputs:
\begin{itemize}
    \item POST and GET methods using an enumeration
    \item The request headers will be in array of strings
    \item The request body will be of string type
    \item The content type response header will be a string type
    \item The response code
    \item Two output pins for knowing if it received a response or there were some kind of network error
    \item The body response will be of string type
\end{itemize}
\noindent
For ease of use there is another class called \texttt{WorldVariables} that inherits \linebreak\texttt{UBlueprintFunctionLibrary} for accessing the server \ac{URL}, this kind of functions are accessible in all Blueprints and \texttt{C++} code.

\section{The backend}
\noindent
The backend will be built in Python using a library called Flask. This library allows associating a function with a specific \ac{URL}.
Since the app is expected to run in a secure local environment for now, no security measures will be implemented, however, this will change in future developments.\\
This table explains all the \ac{API} endpoints Tab.[\ref{tab:API}].\\
Primarily, the server maintains a list of open sessions, each associated with a timestamp and the host's \ac{IP} address, with the \ac{IP} address the client can connect to the host.
If necessary, a new session can be opened. When a session is initiated, a 5 digit code is generated and sent to the host.
If a client submits the correct code, the server will provide the host's IP address.\\
Sessions must be refreshed by the server. Every minute, a thread checks for expired sessions and deletes them.

\begin{table}

    \begin{tblr}{
        colspec = {|l|l|X|l|},
        rows={valign=m},
        row{1}={font=\bfseries}
      }
    \hline
    URL                                            & Methods          & Description                                                                                & {Response\\codes}\\ \hline
    /create\_session                               & POST             & Creates a session if an IP address is provided in the JSON                                 & 200 400\\ \hline
    /join\_session                                 & POST             & Returns the IP address of the host if the code in the JSON corresponding to an open session & 200 404 \\ \hline
    /update\_session/\textless{}code\textgreater{} & GET              & Updates timestamp for session                                                              & 200 404\\ \hline
    /list\_files                                   & GET              & Returns a JSON array with all OBJ files names                                               & 200\\ \hline
    /get\_file/\textless{}filename\textgreater{}  & {GET\\DELETE}      & Returns the OBJ file if exist, or delete it                                               & {200 403\\404}\\ \hline
    /upload                                        & {GET\\OPTIONS}     & {Uploads the OBJ to the server, OPTIONS is needed for JS}                                & 200 400\\ \hline
    \end{tblr}
    \caption{API endpoint}
    \label{tab:API}
\end{table}

\section{Multiplayer functionality}
\noindent
Thanks to \ac{UE} and the VRExpansion plugin, it is relatively simple to establish and maintain consistency between the host and clients.
Initially, the host will create a session so that clients can join. Afterward, the professor using the host \ac{HMD} will announce the session code, which students can enter.
Behind the widget logic, Unreal will attempt to check if the provided \ac{IP} address has an open session on port 7777.
Once connected, the host will decide where to spawn the client, with the location chosen randomly. This logic will run on the \texttt{Game mode} blueprint\\
During the session, it is important to replicate the following:

\begin{itemize}
    \item The location of the controllers and \ac{HMD}, managed by VRExpansion.
    \item The location of the 3D model, managed by Unreal. Since clients can also manipulate the model, it is crucial to allow clients to be authoritative so that the 3D model remains synchronized across all \ac{HMD}s.
    \item The toggle state of the laser pointer, so that all players can see it. However, it is unnecessary to replicate the laser spline, as it can be calculated directly on the device.
    \item Clients teleportation is another important aspect. When the host decides to enter center mode, clients will be teleported to the center. Since only the owning player controller can replicate their own character, the owning clients are responsible for teleporting their characters to the center or to predefined steps.
\end{itemize}


\begin{figure}[ht]
    \includegraphics[width =\textwidth]{diagramTimeNetwork.eps}
    \caption{Network time diagram}
    \label{fig:networkTime}
\end{figure}

\paragraph{About center mode}
Center mode can be enabled by the host via the side menu. This will teleport all players to the center and disable the visualization of controllers and \ac{HMD} meshes for non-owning players. Additionally, it is important to teleport the clients to the same location on the lecture hall steps to avoid creating confusion among students.
To achieve this, when a new player joins the session, their starting point will be saved in their own \texttt{player controller}, making it easily accessible.

\section{Virtual Environment Development}
\noindent
Creating a 3D virtual environment is a complex task that requires the collaboration of multiple individuals with diverse skill sets, such as level designers, concept artists, and 3D artists.
An easier solution is to use freely available assets from the Unreal Marketplace. The asset pack I found, called Assetsville Town\footnote{\url{https://www.fab.com/listings/fd558d8c-bd7e-461f-8449-a7cc9c277078}}.
These assets are ideal for use with the \ac{HMD} due to their low-poly nature, including models such as chairs, books, blackboards, general office appliances and tile sets for creating various environments.
Additionally, the pack provides a wide range of materials and textures for further customization.
In the following sections, I will explain the two environments developed for the application.

\subsection{Main menu}
\noindent
The main menu Fig.[\ref{fig:mainmenuRoom}] is the first interface the user encounters upon entering the application. A table in the virtual environment will display the main menu \ac{UI} widget, allowing the user to create or join sessions.
Beside it, another table will host the \texttt{3DModelPicker} widget, enabling users to interact with individual 3D models.

\begin{figure}[ht]
    \centering
    \includegraphics[width =\textwidth]{vrScreenshot/mainmenu.png}
    \caption{Main menu screenshots}
    \label{fig:mainmenuRoom}
\end{figure}
\subsection{Classroom}
\noindent
The classroom Fig.[\ref{fig:classroom}] will be designed as a lecture hall, with students seated on steps to ensure a clear view of the 3D models, while the professor will be positioned in the middle.
A blackboard at the center will display the session's code, and the bottom-right corner will indicate whether the center mode is enabled.

\begin{figure}[ht]
    \centering
    \includegraphics[width =\textwidth]{vrScreenshot/classroom.jpg}
    \caption{Classroom screenshots}
    \label{fig:classroom}
\end{figure}

\section{Web app}
\noindent
As described in the requirements, there is a need to display different 3D models from time to time. This functionality will be achieved through a React web app build.\\
The main functionalities are:

\begin{itemize}
    \item Upload 3D models
    \item Visualize already uploaded models
    \item Delete 3D models
    \item Responsive design
\end{itemize}
\noindent
The \ac{UI} will be built using MUI\footnote{\url{https://mui.com}}, along with a free template provided by them.\\
For 3D visualization, there is a library called React Three Fiber\footnote{\url{https://r3f.docs.pmnd.rs}}, which is essentially a wrapper around Three.js\footnote{\url{https://threejs.org}}.
React Three Fiber was chosen because it easily supports the visualization and caching of 3D models, as well as vertex colors for OBJ files. The only issue is that it natively displays OBJ models with \texttt{Y-up} orientation. To resolve this, the model simply needs to be rotated on the \texttt{X} axis by -90°.

\begin{figure}[ht]
    \centering
    \includegraphics[width =\textwidth]{mockups.png}
    \caption{Web app mockups}
    \label{fig:mockups}
\end{figure}